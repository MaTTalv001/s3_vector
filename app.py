import streamlit as st
import boto3
import json
import uuid
from datetime import datetime
import numpy as np
import re

# ====== è¨­å®šã®èª­ã¿è¾¼ã¿ ======
try:
    AWS_REGION = st.secrets["aws"]["region"]
    BUCKET_NAME = st.secrets["aws"]["bucket_name"]
    INDEX_NAME = st.secrets["aws"]["index_name"]
    EMBEDDING_MODEL_ID = st.secrets["bedrock"]["embedding_model_id"]
except KeyError as e:
    st.error(f"âŒ è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    st.error("`.streamlit/secrets.toml`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    st.stop()

# ====== AWS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ======
s3vectors = boto3.client("s3vectors", region_name=AWS_REGION)
bedrock = boto3.client("bedrock-runtime", region_name=AWS_REGION)

# ====== TITANåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ ======
def generate_embedding(text: str):
    """Amazon Titan v2ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ (1024æ¬¡å…ƒ)"""
    response = bedrock.invoke_model(
        modelId=EMBEDDING_MODEL_ID,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json"
    )
    return json.loads(response["body"].read())["embedding"]

# ====== queryVectorãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ ======
def fix_query_vector_format(embedding):
    """S3 Vectorsç”¨ã« float32 ã¸å¤‰æ›"""
    if isinstance(embedding, np.ndarray):
        embedding = embedding.astype(np.float32).tolist()
    return {"float32": [float(x) for x in embedding]}

# ====== ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² ======
def chunk_markdown_by_h2(text):
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ ## è¦‹å‡ºã—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    # ##ã§å§‹ã¾ã‚‹è¡Œã§åˆ†å‰²
    chunks = re.split(r'\n(?=##\s)', text)
    
    processed_chunks = []
    for i, chunk in enumerate(chunks):
        chunk = chunk.strip()
        if not chunk:
            continue
            
        # ãƒãƒ£ãƒ³ã‚¯ãŒé•·ã™ãã‚‹å ´åˆã¯æ–‡å­—æ•°ã§å†åˆ†å‰²
        max_chunk_size = 1000  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ¶é™ã‚’è€ƒæ…®
        if len(chunk) > max_chunk_size:
            # æ®µè½å˜ä½ã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
            paragraphs = chunk.split('\n\n')
            current_chunk = ""
            
            for paragraph in paragraphs:
                if len(current_chunk + paragraph) > max_chunk_size and current_chunk:
                    processed_chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            
            if current_chunk.strip():
                processed_chunks.append(current_chunk.strip())
        else:
            processed_chunks.append(chunk)
    
    return processed_chunks

# ====== è¦‹å‡ºã—ã‚’æŠ½å‡º ======
def extract_heading(chunk_text):
    """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¦‹å‡ºã—ï¼ˆ##ï¼‰ã‚’æŠ½å‡º"""
    lines = chunk_text.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('## '):
            return line[3:].strip()  # "## "ã‚’é™¤å»
        elif line.startswith('##'):
            return line[2:].strip()   # "##"ã‚’é™¤å»
    return "è¦‹å‡ºã—ãªã—"

# ====== ãƒ™ã‚¯ãƒˆãƒ«ç™»éŒ² ======
def upload_markdown_as_vectors(text: str):
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã€å„ãƒãƒ£ãƒ³ã‚¯ã‚’åŸ‹ã‚è¾¼ã¿â†’S3 Vectorsã«ç™»éŒ²"""
    chunks = chunk_markdown_by_h2(text)
    
    vectors = []
    for i, chunk in enumerate(chunks):
        embedding = generate_embedding(chunk)
        
        # è¦‹å‡ºã—ã‚’æŠ½å‡º
        heading = extract_heading(chunk)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™ï¼ˆ2048ãƒã‚¤ãƒˆåˆ¶é™å¯¾å¿œï¼‰
        max_metadata_text = 400  # è¦‹å‡ºã—æƒ…å ±ã‚‚å«ã‚ã‚‹ãŸã‚å°‘ã—å°ã•ã
        truncated_text = chunk[:max_metadata_text]
        if len(chunk) > max_metadata_text:
            truncated_text += "..."
        
        vectors.append({
            "key": f"{uuid.uuid4()}",
            "data": {"float32": embedding},
            "metadata": {
                "source_text": truncated_text,
                "heading": heading,
                "timestamp": datetime.now().isoformat(),
                "chunk_index": i,
                "full_length": len(chunk)
            }
        })

    s3vectors.put_vectors(
        vectorBucketName=BUCKET_NAME,
        indexName=INDEX_NAME,
        vectors=vectors
    )
    return len(vectors)

# ====== æ¤œç´¢ ======
def semantic_search(query: str, top_k=5):
    query_embedding = generate_embedding(query)
    query_vector = fix_query_vector_format(query_embedding)

    results = s3vectors.query_vectors(
        vectorBucketName=BUCKET_NAME,
        indexName=INDEX_NAME,
        queryVector=query_vector,
        topK=top_k,
        returnDistance=True,
        returnMetadata=True
    )

    # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«å¤‰æ›
    for v in results["vectors"]:
        v["similarity"] = 1.0 - v["distance"]
    return results["vectors"]

# ====== Streamlit UI ======
st.title("S3 Vectors ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ¤œç´¢ãƒ‡ãƒ¢")

# è¨­å®šæƒ…å ±ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
st.sidebar.header("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
mode = st.sidebar.radio("æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„", ["ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç™»éŒ²", "ãƒ†ã‚­ã‚¹ãƒˆç™»éŒ²", "æ¤œç´¢"])

if mode == "ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç™»éŒ²":
    st.subheader("ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ç™»éŒ²")
    uploaded_file = st.file_uploader("ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["md", "txt"])
    
    if uploaded_file is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã¿
        text_content = uploaded_file.read().decode('utf-8')
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ©Ÿèƒ½
        if st.button("å†…å®¹ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
            st.write("### ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.text(text_content[:1000])
            st.write(f"**ç·æ–‡å­—æ•°:** {len(text_content)}")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            chunks = chunk_markdown_by_h2(text_content)
            st.write(f"**äºˆæƒ³ãƒãƒ£ãƒ³ã‚¯æ•°:** {len(chunks)}")
            
            st.write("### ğŸ“‹ ãƒãƒ£ãƒ³ã‚¯è¦‹å‡ºã—ãƒªã‚¹ãƒˆ")
            for i, chunk in enumerate(chunks[:10]):  # æœ€åˆã®10å€‹ã ã‘è¡¨ç¤º
                heading = extract_heading(chunk)
                st.write(f"{i+1}. {heading}")
            if len(chunks) > 10:
                st.write(f"... ãŠã‚ˆã³ {len(chunks) - 10} å€‹ã®è¿½åŠ ãƒãƒ£ãƒ³ã‚¯")
        
        if st.button("ãƒ™ã‚¯ãƒˆãƒ«ç™»éŒ²"):
            if text_content.strip():
                n_chunks = upload_markdown_as_vectors(text_content)
                st.success(f"âœ… {n_chunks} ãƒãƒ£ãƒ³ã‚¯ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
            else:
                st.warning("âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ãŒç©ºã§ã™")

elif mode == "ãƒ†ã‚­ã‚¹ãƒˆç™»éŒ²":
    st.subheader("ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥å…¥åŠ›")
    st.write("ğŸ’¡ `## è¦‹å‡ºã—` å½¢å¼ã§æ›¸ãã¨ã€è¦‹å‡ºã—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã•ã‚Œã¾ã™")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
    with open("data/sample_text.md", "r", encoding="utf-8") as f:
        sample_content = f.read()

    # ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºèªç”¨ã«è¡¨ç¤º
    if st.button("ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º"):
        st.markdown(sample_content)

    # ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚©ãƒ¼ãƒ ã«ã‚»ãƒƒãƒˆ
    if st.button("ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨"):
        st.session_state.markdown_input = sample_content

    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ æœ¬ä½“ï¼‰
    text_input = st.text_area(
        "ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›",
        value=st.session_state.get("markdown_input", ""),
        height=300
)

    
    if st.button("ãƒ™ã‚¯ãƒˆãƒ«ç™»éŒ²"):
        if text_input.strip():
            n_chunks = upload_markdown_as_vectors(text_input.strip())
            st.success(f"âœ… {n_chunks} ãƒãƒ£ãƒ³ã‚¯ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
        else:
            st.warning("âš ï¸ ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã¯ç™»éŒ²ã§ãã¾ã›ã‚“")

elif mode == "æ¤œç´¢":
    st.subheader("æ¤œç´¢")
    query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›")
    top_k = st.slider("å–å¾—ä»¶æ•°", 1, 10, 5)

    if st.button("æ¤œç´¢å®Ÿè¡Œ"):
        if query.strip():
            results = semantic_search(query, top_k)
            st.write("### ğŸ” æ¤œç´¢çµæœ")
            
            if not results:
                st.warning("âš ï¸ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                for i, r in enumerate(results):
                    st.markdown(f"### çµæœ {i+1}")
                    st.markdown(f"**ã‚¹ã‚³ã‚¢:** {r['similarity']:.4f}")
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¡¨ç¤º
                    metadata = r.get("metadata", {})
                    heading = metadata.get('heading', 'è¦‹å‡ºã—ãªã—')
                    st.markdown(f"**ğŸ“‚ è¦‹å‡ºã—:** {heading}")
                    st.write(f"**ãƒãƒ£ãƒ³ã‚¯ç•ªå·:** {metadata.get('chunk_index', 'N/A')}")
                    st.write(f"**ç™»éŒ²æ—¥æ™‚:** {metadata.get('timestamp', 'N/A')}")
                    st.write(f"**å…ƒã®é•·ã•:** {metadata.get('full_length', 'N/A')}æ–‡å­—")
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
                    source_text = metadata.get("source_text", "")
                    st.write("**å†…å®¹:**")
                    st.markdown(source_text)
                    
                    st.caption(f"Key: {r['key']}")
                    st.divider()
        else:
            st.warning("âš ï¸ ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")